diff --git a/render.py b/render.py
index fc6b82d..0388e39 100644
--- a/render.py
+++ b/render.py
@@ -20,6 +20,8 @@ from utils.general_utils import safe_state
 from argparse import ArgumentParser
 from arguments import ModelParams, PipelineParams, get_combined_args
 from gaussian_renderer import GaussianModel
+import numpy as np
+from time import time
 
 def render_set(model_path, name, iteration, views, gaussians, pipeline, background):
     render_path = os.path.join(model_path, name, "ours_{}".format(iteration), "renders")
@@ -34,6 +36,19 @@ def render_set(model_path, name, iteration, views, gaussians, pipeline, backgrou
         torchvision.utils.save_image(rendering, os.path.join(render_path, '{0:05d}'.format(idx) + ".png"))
         torchvision.utils.save_image(gt, os.path.join(gts_path, '{0:05d}'.format(idx) + ".png"))
 
+        # Render multiple times to get a better estimate of FPS
+    runtimes = []
+    for _ in range(100):
+        for idx, view in enumerate(views):
+            if idx == 0:time1 = time()
+            rendering = render(view, gaussians, pipeline, background)["render"]
+        time2=time()
+        runtime=time2-time1
+        runtimes.append(runtime)
+    # get average of 5 smallest runtimes
+    runtimes = np.mean(np.sort(np.array(runtimes))[50:55])
+    print("Average FPS:",(len(views)-1)/runtimes)
+
 def render_sets(dataset : ModelParams, iteration : int, pipeline : PipelineParams, skip_train : bool, skip_test : bool):
     with torch.no_grad():
         gaussians = GaussianModel(dataset.sh_degree)
diff --git a/scene/dataset_readers.py b/scene/dataset_readers.py
index 2a6f904..6d2d16c 100644
--- a/scene/dataset_readers.py
+++ b/scene/dataset_readers.py
@@ -181,7 +181,7 @@ def readCamerasFromTransforms(path, transformsfile, white_background, extension=
 
     with open(os.path.join(path, transformsfile)) as json_file:
         contents = json.load(json_file)
-        fovx = contents["camera_angle_x"]
+        fovx = contents["fl_x"]
 
         frames = contents["frames"]
         for idx, frame in enumerate(frames):
@@ -209,16 +209,15 @@ def readCamerasFromTransforms(path, transformsfile, white_background, extension=
             arr = norm_data[:,:,:3] * norm_data[:, :, 3:4] + bg * (1 - norm_data[:, :, 3:4])
             image = Image.fromarray(np.array(arr*255.0, dtype=np.byte), "RGB")
 
-            fovy = focal2fov(fov2focal(fovx, image.size[0]), image.size[1])
-            FovY = fovy 
-            FovX = fovx
+            FovY = focal2fov(fovx, image.size[1]) 
+            FovX = focal2fov(fovx, image.size[0])
 
             cam_infos.append(CameraInfo(uid=idx, R=R, T=T, FovY=FovY, FovX=FovX, image=image,
                             image_path=image_path, image_name=image_name, width=image.size[0], height=image.size[1]))
             
     return cam_infos
 
-def readNerfSyntheticInfo(path, white_background, eval, extension=".png"):
+def readNerfSyntheticInfo(path, white_background, eval, extension=""):
     print("Reading Training Transforms")
     train_cam_infos = readCamerasFromTransforms(path, "transforms_train.json", white_background, extension)
     print("Reading Test Transforms")
diff --git a/train.py b/train.py
index 5d819b3..08f4ebb 100644
--- a/train.py
+++ b/train.py
@@ -22,6 +22,7 @@ from tqdm import tqdm
 from utils.image_utils import psnr
 from argparse import ArgumentParser, Namespace
 from arguments import ModelParams, PipelineParams, OptimizationParams
+
 try:
     from torch.utils.tensorboard import SummaryWriter
     TENSORBOARD_FOUND = True
